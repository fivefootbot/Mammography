{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import keras\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import (Input, \n",
    "                                     Conv2D,\n",
    "                                     GlobalMaxPooling2D,\n",
    "                                     MaxPooling2D,\n",
    "                                     Dropout, \n",
    "                                     Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgShape = (256, 256, 1)\n",
    "folder = '/mnt/c/Users/vange/OneDrive - Tennessee Tech University/Desktop/Fall 23, Spring 24/4260/mammography'\n",
    "\n",
    "\n",
    "class BatchCreator(Sequence):\n",
    "\n",
    "    def __init__(self, data, dataType, batchSize, workers = 8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data = data\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        if dataType == 1:\n",
    "            self.dataType = 'train'\n",
    "            \n",
    "        else:\n",
    "            self.dataType = 'test'\n",
    "\n",
    "\n",
    "    # returns the number of batches in the dataset \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batchSize)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # calculates the beginning and end indices for the batch based on the batch size and the index\n",
    "        start = index * self.batchSize\n",
    "        end = min(start + self.batchSize, len(self.data))\n",
    "        \n",
    "        # initializes arrays to hold the input images and labels for the batch\n",
    "        inputImages = np.zeros((self.batchSize, ) + imgShape)        \n",
    "        labels = np.zeros((self.batchSize, 1))\n",
    "\n",
    "        # iterates over the indices of the current batch, stops if the index exceeds the length of the dataset\n",
    "        for i, idx in enumerate(range(start, end)):          \n",
    "            if index >= len(self.data):\n",
    "                break\n",
    "                \n",
    "            patientID = self.data.iloc[idx]['patient_id']\n",
    "            imgID = self.data.iloc[idx]['image_id']\n",
    "            \n",
    "            fileName = os.path.join(folder, f\"{self.dataType}_images\", str(patientID), f\"{imgID}.dcm\")\n",
    "            image = pydicom.dcmread(fileName)\n",
    "            images = image.pixel_array\n",
    "\n",
    "            # normalizing pixel values [0, 1]\n",
    "            images = minmax_scale(images)\n",
    "\n",
    "            # Many images are in MONOCHROME1 format, we however need all in MONOCHROME2 format since 0 is black instead\n",
    "            # of white like in MONOCRHOME1. This ensures that our dark areas in the images are actually dark and that\n",
    "            # the bright areas are actually bright, in its original form, the opposite is true.\n",
    "            if image.PhotometricInterpretation == \"MONOCHROME1\":\n",
    "                images = 1 - images\n",
    "\n",
    "            # resizes the image using nearest-neighbor interpolation and converts it to a nparray\n",
    "            images = tf.image.resize(images = np.expand_dims(images, axis = -1), size = imgShape[:-1], method='nearest').numpy()\n",
    "\n",
    "            # assigns the processed image to the inputImages array at index i\n",
    "            inputImages[i, :, :, 0] = images.squeeze()\n",
    "            \n",
    "            if self.dataType == 'train':\n",
    "                # assigns the label to the labels array at index i\n",
    "                labels[i] = self.data.iloc[idx]['cancer']\n",
    "            \n",
    "        if self.dataType == 'train':\n",
    "            return (inputImages, labels)\n",
    "        \n",
    "        else: \n",
    "            return inputImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2316"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = pd.read_csv(\"/mnt/c/Users/vange/OneDrive - Tennessee Tech University/Desktop/Fall 23, Spring 24/4260/mammography/train.csv\")\n",
    "newTrainData = []\n",
    "\n",
    "# counts each unique cancer value\n",
    "cancerCount = trainData.cancer.value_counts()\n",
    "\n",
    "# prevents oversampling of patients with and without cancer, we use all available true cancer patients\n",
    "# and sample the rest of the dataset to get the same amount of noncancerous patients\n",
    "for i, placeholder in cancerCount.items():\n",
    "    filteredData = trainData[trainData['cancer'] == i]\n",
    "    sampledData = filteredData.sample(cancerCount[1])\n",
    "    newTrainData.append(sampledData)\n",
    "\n",
    "trainDF = pd.concat(newTrainData)\n",
    "\n",
    "# gives 2316, we have 1158 true cancer patients, so we grab 1158 false cancer patients\n",
    "len(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1852, 464)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xVal = train_test_split(trainDF, test_size = 0.20, random_state = 1)\n",
    "len(xTrain), len(xVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gendTrain = BatchCreator(xTrain, 1, 15)\n",
    "gendVal = BatchCreator(xVal, 1, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a dropout of 0.35 - 0.4 and compare to 0.3, 0.5 is not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 252, 252, 32)      832       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 248, 248, 64)      51264     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 124, 124, 64)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 124, 124, 64)      0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 120, 120, 64)      102464    \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 116, 116, 128)     204928    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 58, 58, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 58, 58, 128)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 54, 54, 128)       409728    \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 50, 50, 256)       819456    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 25, 25, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 25, 25, 256)       0         \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Gl  (None, 256)               0         \n",
      " obalMaxPooling2D)                                               \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1596929 (6.09 MB)\n",
      "Trainable params: 1596929 (6.09 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "imgShape = (256, 256, 1)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape = imgShape))\n",
    "\n",
    "# Start with 32 filters and scale by 2 each layer, except for the layer following the next each time\n",
    "# MaxPooling2d will downsample helping reduce computations, can also help overfitting and model efficiency\n",
    "# Dropout randomly sets 30% of input to 0 to reduce overfitting, repeat these multiple times\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), activation = \"relu\"))\n",
    "model.add(Conv2D(64, (5, 5), activation = \"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), activation = \"relu\"))\n",
    "model.add(Conv2D(128, (5, 5), activation = \"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (5, 5), activation = \"relu\"))\n",
    "model.add(Conv2D(256, (5, 5), activation = \"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Dense layer used to learn non-linear combinations of the features\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer, sigmoid activation function for binary classification\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# using recall and precision with different thresholds to see performance\n",
    "model.compile(optimizer = Adam(learning_rate = 0.0005), loss = 'binary_crossentropy',\n",
    "              metrics = [tf.keras.metrics.BinaryAccuracy(threshold = 0.5),\n",
    "                         tf.keras.metrics.Recall(thresholds = [0.4, 0.5, 0.6]),\n",
    "                         tf.keras.metrics.Precision(thresholds = [0.4, 0.5, 0.6])])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = quantified value of how well the model is predicting vs the actual value, we want a low number\n",
    "# binary accuracy = percentage of correct predictions\n",
    "# recall = % of true + over all + predictions (true and false +) accuracy of the + predictions\n",
    "# precision = % of true + out of all + (true + and false -) ability of model to find all + samples\n",
    "# f1score = balance between recall and precision, we want high number\n",
    "\n",
    "\n",
    "history = model.fit(gendTrain, validation_data = gendVal, epochs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()\n",
    "f1_scores = []\n",
    "\n",
    "# calculate the F1 score by iterating over the precision and recall values in the model history\n",
    "# change precision_# to whatever numbers shows in the training history of model.fit\n",
    "for precision, recall in zip(history.history['precision_8'], history.history['recall_8']):\n",
    "    f1Score = 2 * (precision * recall) / (precision + recall)\n",
    "    f1_scores.append(f1Score)\n",
    "\n",
    "max(f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 5)\n",
    "\n",
    "plt.plot(epochs, history.history['loss'], 'go', label = 'Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'b', label = 'Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, history.history['binary_accuracy'], 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, history.history['val_binary_accuracy'], 'r', label = 'Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
